---
title: "Novel vs. repeated choice project: DDM state space outputs"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
theme_set(theme_bw())
helpers_path = paste0(here(),'/analysis/helpers/')
# source(paste0(helpers_path, '01_clean_behavioral_data.R'))
# rm(data_bc_clean)
fig_out_path = paste0(here(), '/outputs/fig/')
cpu_eaters_path = '/Users/zeynepenkavi/CpuEaters/NovelVsRepeated/behavior/analysis/helpers/cluster_scripts/'
```

# Grid search results

Fit grid search for one subject (601).  

Value was normalized for each job (i.e. each day for each stimulus type) to range between -1 and 1.

Grid from `save_ddm_grid.R`

```
d = seq(0.01, .1, .01)
sigma = seq(0.01, .1, .01)
ndt = seq(200, 400, 100)
bias = seq(-.1, .1, .1)
barrierDecay = c(0, .001, .01, .02)
```

Not parallelized over grid. Parallelized only over day and type. So there were 22 jobs (11 days x 2 types). Each job computed the likelihood of observed data for 3600 parameter combinations (10 x 10 x 3 x 3 x 4) unless anything errored out/job was cancelled for taking too long.  

Boundary separation fixed at 2 (but allowed to decay if `barrierDecay` != 0).  

Using state-space approach with an approximate step size of 0.01.  

Note that there won't be any posterior distributions in this approach.  

Output for a single job looks like:   

```{r}
grid_search_outputs = list.files(paste0(cpu_eaters_path, 'ddm/grid_search_out'))

tmp = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', grid_search_outputs[1]))

tmp
```

How to examine a single job's output/parameter estimates for one day and stim type

The simplest thing is too just get the parameter combination with the highest likelihood/smallest nll.  

```{r}
tmp %>%
  arrange(nll) %>%
  slice(1)
```

But is this parameter combination obviously better than the others? What does the distribution of the likelihood look like?

The whole distribution shows some very large nll's, i.e. some very bad fits to data.

```{r}
tmp %>% 
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30)
```
Half of the data has a nll larger than 3809.

```{r}
summary(tmp$nll)
```

So the distribution of likelihoods for the better fitting half of the parameter combination looks like:

```{r}
tmp %>% 
  filter(nll < 3809) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30)
```
Are there clearly terrible parameter values for the combinations that are in the better fitting half?

```{r}
tmp %>%
  filter(nll < 3809) %>%
  select(-subnum, -day, -type, -model) %>%
  gather(key, value, -nll) %>%
  ggplot(aes(as.factor(value), nll))+
  geom_boxplot()+
  facet_wrap(~key, scales = 'free')+
  labs(x = "")
```
Things to check for each job:
- Distribution of likelihoods
- Bad areas of parameter space
- Number of failed jobs

```{r}
all_pars = tibble()

for(i in grid_search_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', i))
  all_pars = rbind(all_pars, cur_out)
}

rm(cur_out, i)
```

There are some failed jobs for HT day 1 and day 2. All others seem to have completed likelihood calculation for all 3600 parameter combinations.

```{r}
all_pars %>%
  group_by(day, type) %>%
  summarise(count = n(), .groups = "keep") %>%
  arrange(count)
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30) +
  facet_grid(type ~ day) +
  labes(title = "Full NLL distributions")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30) +
  facet_grid(type ~ day, scales = "free") +
  labs(title = "Lower 50% NLL distributions")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "d") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each d collapsed over other parameters (for 50% best NLLs)")
```
```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "sigma") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each sigma collapsed over other parameters (for 50% best NLLs)")
```


```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "nonDecisionTime") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each ndt collapsed over other parameters (for 50% best NLLs)")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "bias") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each bias collapsed over other parameters (for 50% best NLLs)")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "barrierDecay") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each barrierDecay collapsed over other parameters (for 50% best NLLs)")
```

Differences in best fitting parameters across stim types and days

```{r}
best_pars = tibble()

for(i in grid_search_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', i))
  cur_best = cur_out %>%
    arrange(nll) %>%
    slice(1)
  best_pars = rbind(best_pars, cur_best)
}

rm(cur_out, cur_best, i)
```

Notable things:

- Best fitting parameters have barrierDecay>0
- Barrier decay is either the same for the two conditions or higher for HT
- Barrier decay is higher on later days

- Bias is mostly 0 (--> consider fixing it?)
- When bias is not 0, it's positive for HT (yes) and negative for RE (no)

- Drift rates increase across days
- Drift rates are higher for HT

- Non decision times are more often higher for HT (unexpected)
- Generally constant across days (good)

- Noise is higher for HT half the time and higher for RE for the other half
- No clear pattern of increase or decrease over days

```{r}
best_pars %>%
  select(-subnum, -model, -nll) %>%
  gather(key, value, -day, -type) %>%
  ggplot(aes(as.factor(day), value, color = type))+
  geom_point(alpha = .5)+
  facet_wrap(~key, scales = "free")+
  scale_color_brewer(palette = "Dark2")+
  labs(x = "Day")
```

# Optim out results

Fit for 3 subjects (601, 609 and 611) for both conditions and all days.

Value was normalized for each job (i.e. each day for each stimulus type) to range between -1 and 1.

Ideally, the algorithm should converge on the same/similar parameters for each job (fit for subject, day, type).

But since I'm not sure how robust it is I had planned to have it start from each of the parameter combinations used for the grid search. This would have meant trying 3600 different starting values and then checking if the algorithm converges (after max 500 iterations) to the same place.  

It still doesn't give full posteriors but at least we would have had a little more support for the converged value.  
I did not parallelize across starting values. So this ended up taking forever and I only have a few outputs for each job (subject, day, type) instead of the full 3600.

```{r}
optim_outputs = list.files(paste0(cpu_eaters_path, 'ddm/optim_out'))

tmp = read.csv(paste0(cpu_eaters_path, 'ddm/optim_out/', optim_outputs[1]))

tmp
```

```{r}
all_optim = tibble()

for(i in optim_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/optim_out/', i))
  all_optim = rbind(all_optim, cur_out)
}

rm(cur_out, i)

```

```{r}
all_optim %>%
  arrange(barrierDecay)

all_optim %>%
  filter(barrierDecay > 0 & barrierDecay < 1) %>%
  arrange(-barrierDecay)
```

How far are the converged parameters from their starting value? 

```{r}
unique(all_optim$start_barrierDecay)
unique(all_optim$start_bias)
unique(all_optim$start_d)
unique(all_optim$start_nonDecisionTime)
unique(all_optim$start_sigma)
```
Things seem to have moved from their starting values.

```{r}
all_optim %>%
  filter(barrierDecay < 1 & barrierDecay > 0) %>%
  filter(bias < 1) %>%
  mutate(diff_barrierDecay = start_barrierDecay - barrierDecay,
         diff_bias = start_bias - bias,
         diff_d = start_d - d,
         diff_nonDecisionTime = start_nonDecisionTime - nonDecisionTime,
         diff_sigma = start_sigma - sigma) %>%
  # select(d, sigma, nonDecisionTime, bias, barrierDecay) %>%
  select(diff_d, diff_sigma, diff_nonDecisionTime, diff_bias, diff_barrierDecay) %>%
  gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram(bins = 20, alpha = .5) + 
  facet_wrap(~key, scales = "free")
```

How many iterations did it take to converge per starting value (collapsing across subjects, days, types)

```{r}
all_optim %>%
  ggplot(aes(optim_iters)) + 
  geom_histogram(alpha = .5, bins = 20)
```

How different are the converged estimates from each other per job?

```{r}
all_optim %>%
  select(subnum, day, type, bias, barrierDecay, d, sigma, nonDecisionTime) %>%
  gather(key, value, -subnum, -day, -type) %>%
  group_by(subnum, day, type, key) %>%
  summarise(.groups = "keep",
            sem_par = sd(value)/sqrt(n()))
```

Non sensical estimates?

Negative drift rates, sigmas, barrierDecays, ndt's?

```{r}

```

How close are the best estimates from optim to the single subject grid search best parameters?

```{r}

```
