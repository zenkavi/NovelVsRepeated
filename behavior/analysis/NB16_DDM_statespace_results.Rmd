---
title: "Novel vs. repeated choice project: DDM state space outputs"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: 'hide'
---

Set up environment and load in data

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(here)
theme_set(theme_bw())
helpers_path = paste0(here(),'/analysis/helpers/')
# source(paste0(helpers_path, '01_clean_behavioral_data.R'))
# rm(data_bc_clean)
fig_out_path = paste0(here(), '/outputs/fig/')
cpu_eaters_path = '/Users/zeynepenkavi/CpuEaters/NovelVsRepeated/behavior/analysis/helpers/cluster_scripts/'
```

# Grid search results

Fit grid search for one subject (601).  

Value was normalized for each job (i.e. each day for each stimulus type) to range between -1 and 1.

Grid from `save_ddm_grid.R`

```
d = seq(0.01, .1, .01)
sigma = seq(0.01, .1, .01)
ndt = seq(200, 400, 100)
bias = seq(-.1, .1, .1)
barrierDecay = c(0, .001, .01, .02)
```

Not parallelized over grid. Parallelized only over day and type. So there were 22 jobs (11 days x 2 types). Each job computed the likelihood of observed data for 3600 parameter combinations (10 x 10 x 3 x 3 x 4) unless anything errored out/job was cancelled for taking too long.  

Boundary separation fixed at 2 (but allowed to decay if `barrierDecay` != 0).  

Using state-space approach with an approximate step size of 0.01.  

Note that there won't be any posterior distributions in this approach.  

## Single job output

Output for a single job looks like:   

```{r}
grid_search_outputs = list.files(paste0(cpu_eaters_path, 'ddm/grid_search_out'))

tmp = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', grid_search_outputs[1]))

tmp
```

How to examine a single job's output/parameter estimates for one day and stim type

The simplest thing is too just get the parameter combination with the highest likelihood/smallest nll.  

```{r}
tmp %>%
  arrange(nll) %>%
  slice(1)
```

But is this parameter combination obviously better than the others? What does the distribution of the likelihood look like?

The whole distribution shows some very large nll's, i.e. some very bad fits to data.

```{r}
tmp %>% 
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30)
```
Half of the data has a nll larger than 3809.

```{r}
summary(tmp$nll)
```

So the distribution of likelihoods for the better fitting half of the parameter combination looks like:

```{r}
tmp %>% 
  filter(nll < 3809) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30)
```
Are there clearly terrible parameter values for the combinations that are in the better fitting half?

```{r}
tmp %>%
  filter(nll < 3809) %>%
  select(-subnum, -day, -type, -model) %>%
  gather(key, value, -nll) %>%
  ggplot(aes(as.factor(value), nll))+
  geom_boxplot()+
  facet_wrap(~key, scales = 'free')+
  labs(x = "")
```

## All jobs' outputs

Things to check for each job:
- Distribution of likelihoods
- Bad areas of parameter space
- Number of failed jobs

```{r}
all_pars = tibble()

for(i in grid_search_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', i))
  all_pars = rbind(all_pars, cur_out)
}

rm(cur_out, i)
```

There are some failed jobs for HT day 1 and day 2. All others seem to have completed likelihood calculation for all 3600 parameter combinations.

```{r}
all_pars %>%
  group_by(day, type) %>%
  summarise(count = n(), .groups = "keep") %>%
  arrange(count)
```

### NLL distributions

```{r}
all_pars %>%
  group_by(day, type) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30) +
  facet_grid(type ~ day) +
  labs(title = "Full NLL distributions")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  ggplot(aes(nll)) +
  geom_histogram(alpha = .5, bins = 30) +
  facet_grid(type ~ day, scales = "free") +
  labs(title = "Lower 50% NLL distributions")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "d") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each d collapsed over other parameters (for 50% best NLLs)")
```
```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "sigma") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each sigma collapsed over other parameters (for 50% best NLLs)")
```


```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "nonDecisionTime") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each ndt collapsed over other parameters (for 50% best NLLs)")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "bias") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each bias collapsed over other parameters (for 50% best NLLs)")
```

```{r}
all_pars %>%
  group_by(day, type) %>%
  mutate(median_nll = median(nll)) %>%
  filter(nll > median_nll) %>%
  select(-subnum, -model) %>%
  gather(key, value, -nll, -day, -type) %>%
  filter(key == "barrierDecay") %>%
  ggplot(aes(as.factor(value), nll))+
  geom_violin()+
  facet_grid(type ~ day, scales = 'free')+
  labs(x = "", title = "NLL distribution for each barrierDecay collapsed over other parameters (for 50% best NLLs)")
```
### Best fitting parameters

Differences in best fitting parameters across stim types and days

```{r}
best_pars = tibble()

for(i in grid_search_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/grid_search_out/', i))
  cur_best = cur_out %>%
    arrange(nll) %>%
    slice(1)
  best_pars = rbind(best_pars, cur_best)
}

rm(cur_out, cur_best, i)
```

Notable things:

- Best fitting parameters have barrierDecay>0
- Barrier decay is either the same for the two conditions or higher for HT
- Barrier decay is higher on later days

- Bias is mostly 0 (--> consider fixing it?)
- When bias is not 0, it's positive for HT (yes) and negative for RE (no)

- Drift rates increase across days
- Drift rates are higher for HT

- Non decision times are more often higher for HT (unexpected)
- Generally constant across days (good)

- Noise is higher for HT half the time and higher for RE for the other half
- No clear pattern of increase or decrease over days

```{r}
best_pars %>%
  select(-subnum, -model, -nll) %>%
  gather(key, value, -day, -type) %>%
  ggplot(aes(as.factor(day), value, color = type))+
  geom_point(alpha = .5)+
  facet_wrap(~key, scales = "free")+
  scale_color_brewer(palette = "Dark2")+
  labs(x = "Day")
```

# Optim out results

Fit for 3 subjects (601, 609 and 611) for both conditions and all days.

Value was normalized for each job (i.e. each day for each stimulus type) to range between -1 and 1.

Ideally, the algorithm should converge on the same/similar parameters for each job (fit for subject, day, type).

But since I'm not sure how robust it is I had planned to have it start from each of the parameter combinations used for the grid search. This would have meant trying 3600 different starting values and then checking if the algorithm converges (after max 500 iterations) to the same place.  

It still doesn't give full posteriors but at least we would have had a little more support for the converged value.  
I did not parallelize across starting values. So this ended up taking forever and I only have a few outputs for each job (subject, day, type) instead of the full 3600.

```{r}
optim_outputs = list.files(paste0(cpu_eaters_path, 'ddm/optim_out'))

tmp = read.csv(paste0(cpu_eaters_path, 'ddm/optim_out/', optim_outputs[1]))

tmp
```

## All jobs' outputs

```{r}
all_optim = tibble()

for(i in optim_outputs){
  cur_out = read.csv(paste0(cpu_eaters_path, 'ddm/optim_out/', i))
  all_optim = rbind(all_optim, cur_out)
}

rm(cur_out, i)

```

How many converged iterations per job? Max = 12, min 4.

```{r}
all_optim %>%
  group_by(day, subnum, type) %>%
  summarise(.groups = "keep",
            max_iter = max(start_num)) %>%
  arrange(-max_iter)
```

Excluding non-sensical values for each paramter is there a converged parameter combination for each job (subject, day, type combination)? Yes.

```{r}
all_optim %>%
  filter(barrierDecay > 0 & barrierDecay < 1) %>%
  filter(bias < 1) %>%
  filter(d > 0) %>%
  filter(sigma > 0) %>%
  filter(nonDecisionTime > 0) %>%
    group_by(day, subnum, type) %>%
  summarise(.groups = "keep",
            max_iter = max(start_num)) %>%
  arrange(-max_iter)
```

```{r}
all_optim_filtered = all_optim %>%  
  filter(barrierDecay > 0 & barrierDecay < 1) %>%
  filter(bias < 1) %>%
  filter(d > 0) %>%
  filter(sigma > 0) %>%
  filter(nonDecisionTime > 0)
```


How far are the converged parameters from their starting value?   

There should have been 10 x 10 x 3 x 3 x 4 starting values.  

```
d = seq(0.01, .1, .01)
sigma = seq(0.01, .1, .01)
ndt = seq(200, 400, 100)
bias = seq(-.1, .1, .1)
barrierDecay = c(0, .001, .01, .02)
```

But instead there is only 1 barrierDecay and bias starting value, 3 drift rate starting values, 2 ndt starting values and 3 noise level starting values.

```{r}
unique(all_optim$start_barrierDecay)
unique(all_optim$start_bias)
unique(all_optim$start_d)
unique(all_optim$start_nonDecisionTime)
unique(all_optim$start_sigma)
```
Filtering converged jobs with non-sensical values does not remove a specific starting value for any of the parameters.

```{r}
unique(all_optim_filtered$start_barrierDecay)
unique(all_optim_filtered$start_bias)
unique(all_optim_filtered$start_d)
unique(all_optim_filtered$start_nonDecisionTime)
unique(all_optim_filtered$start_sigma)
```

### Distance from start value

To check if things have moved from their starting value I plot the histograms for the difference between the converged value and the starting value for each parameter across jobs.  

This only looks at converged jobs that have sensible values for all parameters.

```{r}
all_optim_filtered %>%
  mutate(diff_barrierDecay = start_barrierDecay - barrierDecay,
         diff_bias = start_bias - bias,
         diff_d = start_d - d,
         diff_nonDecisionTime = start_nonDecisionTime - nonDecisionTime,
         diff_sigma = start_sigma - sigma) %>%
  select(diff_d, diff_sigma, diff_nonDecisionTime, diff_bias, diff_barrierDecay) %>%
  gather(key, value) %>%
  ggplot(aes(value))+
  geom_histogram(bins = 20, alpha = .5) + 
  facet_wrap(~key, scales = "free")
```

How many iterations did it take to converge per starting value (collapsing across subjects, days, types)

```{r}
all_optim_filtered %>%
  ggplot(aes(optim_iters)) + 
  geom_histogram(alpha = .5, bins = 20)
```

### Variance in converged estimates

How different are the converged estimates from each other for each job (subject, day, type combination) per starting value?

They can be quite different from each other especially when they do move further away from their starting values. The estimates for barrrierDecay and ndt differ less from each other depending on the converged value but they also don't move as far away from their starting values either.

```{r}
all_optim_filtered %>%
  select(subnum, day, type, bias, barrierDecay, d, sigma, nonDecisionTime) %>%
  gather(key, value, -subnum, -day, -type) %>%
  group_by(subnum, day, type, key) %>%
  ggplot(aes(as.factor(day), value, color = type))+
  geom_point(position = position_dodge(width=.5))+
  facet_grid(key~subnum, scales = "free")+
  theme(legend.position = "bottom")+
  scale_color_brewer(palette = "Dark2")+
  labs(color = "", y = "Converged estimate", x = "Day")
```

```{r}
all_optim_filtered %>%
  mutate(diff_barrierDecay = start_barrierDecay - barrierDecay,
         diff_bias = start_bias - bias,
         diff_d = start_d - d,
         diff_nonDecisionTime = start_nonDecisionTime - nonDecisionTime,
         diff_sigma = start_sigma - sigma) %>%
  select(subnum, day, type, diff_d, diff_sigma, diff_nonDecisionTime, diff_bias, diff_barrierDecay) %>%
  gather(key, value, -subnum, -day, -type) %>%
  group_by(subnum, day, type, key) %>%
  ggplot(aes(as.factor(day), value, color = type))+
  geom_point(position = position_dodge(width=.5))+
  geom_hline(aes(yintercept = 0)) +
  facet_grid(key~subnum, scales = "free")+
  theme(legend.position = "bottom")+
  scale_color_brewer(palette = "Dark2")+
  labs(color = "", y = "Difference between starting and converged value", x = "Day")
```

## Single subject optim and grid search comparison

How close are the best estimates from optim to the single subject (601) grid search best parameters?

```{r}
sub_grid_best = best_pars %>%
  select(-model) %>%
  mutate(fit = "grid")
  
```

```{r}
sub_optim_best = all_optim_filtered %>%
  filter(subnum == 601) %>%
  group_by(subnum, day, type) %>%
  filter(nll == min(nll)) %>%
  select(barrierDecay, bias, d, nonDecisionTime, sigma, nll, subnum, day, type) %>%
  mutate(fit = "optim")
```

### NLL in optim vs grid search

Compare likelihoods of best fitting parameters from two approaches. 

Notable things:

- Model fits RE better than HT for all days.  
- Between the two fitting schemes the likelihood of the best fitting parameter combinations is almost identical.

```{r}
sub_grid_best %>%
  select(nll, subnum, day, type, fit) %>%
  rbind(sub_optim_best %>%
            select(nll, subnum, day, type, fit)) %>%
  spread(fit, nll) %>%
  ggplot(aes(grid, optim, color = type))+
  geom_point()+
  geom_abline(aes(slope = 1, intercept = 0))+
  theme(legend.position = "bottom")+
  scale_color_brewer(palette = "Dark2")+
  labs(color = "", y = "NLL from optim", x = "NLL from grid search")
```

### Parameter estimates for optim vs grid search

For these almost identical nll's are the converged parameters similar? Yes and no. The exact estimates are not identifical but patterns, especially comparing the two conditions are captured.

- The best barrierDecays for RE are often lower than those for HT in both methods.  
- When grid search converges on a non-zero bias the sign of the bias always matches with optim too. When grid search converges on 0 the biggest difference is still smaller than the other best grid search estimates.  
- ndt's are where the largest differences are. Optim estimates much faster ndt's compared to grid search.  
- Drift rates are often larger when using grid search compared to optim. This is possibly somewhat due to the larger ndt's with grid search.  
- The optim sigma estimates range between the two most frequently converged values with grid search ([0.04 - 0.05]).  

```{r}
sub_grid_best %>%
  select(-nll) %>%
  gather(key, value, -subnum, -day, -type, -fit) %>%
  rbind(sub_optim_best %>%
          select(-nll) %>%
          gather(key, value, -subnum, -day, -type, -fit)) %>%
  spread(fit, value) %>%
  ggplot(aes(grid, optim, color = type))+
  geom_point(alpha = .5)+
  geom_abline(aes(intercept = 0, slope = 1))+
    theme(legend.position = "bottom")+
  scale_color_brewer(palette = "Dark2")+
  labs(color = "", y = "Estimate from optim", x = "Estimate grid search")+
  facet_wrap(~key, scales = "free")
```

### Simulated data checks

How much do the exact values of the point estimates matter?  

How different does data generated by the point estimates recovered by the two different methods look from each other AND from true data?  

Source simulation functions.  

```{r}
# Source model definition
source(paste0(helpers_path, '/ddm/yn_ddm.R'))

# Source simulating functions
source(paste0(helpers_path, '/ddm/sim_yn_ddm.R'))

# Create list with the simulator function
sim_trial_list = list("model1" = sim_trial)

# Get data for subject
source(paste0(helpers_path, '01_clean_behavioral_data.R'))
rm(data_bc_clean)

normMax = 1
normMin = -1

data_yn_clean = data_yn_clean %>%
  filter(reference != -99) %>%
  filter(rt > .3 & rt < 5) %>% # discard very long and short RT trials
  group_by(subnum, day, type) %>%
  mutate(possiblePayoff_dmn = possiblePayoff - mean(possiblePayoff)) %>%
  mutate(rawVDiff = possiblePayoff - reference,
         normVDiff =  (normMax - normMin) / (max(rawVDiff) - min(rawVDiff)) * (rawVDiff - max(rawVDiff)) + (normMax) )

rm(normMax, normMin)

sub_data = data_yn_clean %>% filter(subnum == 601)
```


Simulate data using best estimates from grid search

```{r}
sim_grid_best = tibble()

for(i in 1:nrow(sub_grid_best)){
  cur_row = sub_grid_best[i,]
  cur_day = cur_row$day
  cur_type = ifelse(cur_row$type == "HT", 1, 0)
  cur_stims = sub_data %>% filter(day == cur_day & type == cur_type)
  cur_sim = sim_task(stimuli = cur_stims, model_name = "model1", sim_trial_list_ = sim_trial_list, 
                     d = cur_row$d, sigma = cur_row$sigma, nonDecisionTime = cur_row$nonDecisionTime, 
                     bias = cur_row$bias, barrierDecay = cur_row$barrierDecay, 
                     maxIter = 300)
  cur_sim$type = cur_type
  cur_sim$day = cur_day
  cur_sim$true_yesChosen = cur_stims$yesChosen
  cur_sim$true_rt = cur_stims$rt
  sim_grid_best = rbind(sim_grid_best, cur_sim)
  
}

rm(cur_row, cur_sim, cur_stims, i)
```

```{r}
sim_grid_best
```

Simulate data using best parameters from optim

```{r}
sim_optim_best = tibble()

for(i in 1:nrow(sub_optim_best)){
  cur_row = sub_optim_best[i,]
  cur_day = cur_row$day
  cur_type = ifelse(cur_row$type == "HT", 1, 0)
  cur_stims = sub_data %>% filter(day == cur_day & type == cur_type)
  cur_sim = sim_task(stimuli = cur_stims, model_name = "model1", sim_trial_list_ = sim_trial_list, 
                     d = cur_row$d, sigma = cur_row$sigma, nonDecisionTime = cur_row$nonDecisionTime, 
                     bias = cur_row$bias, barrierDecay = cur_row$barrierDecay, 
                     maxIter = 300)
  cur_sim$type = cur_type
  cur_sim$day = cur_day
  cur_sim$true_yesChosen = cur_stims$yesChosen
  cur_sim$true_rt = cur_stims$rt
  sim_optim_best = rbind(sim_optim_best, cur_sim)
  
}

rm(cur_row, cur_sim, cur_stims, i)
```

```{r}
sim_optim_best
```

#### Plot simulated data

Whether they are in the same or different layers doesn't matter BUT make sure to use the same geom for both true and simulated data! Otherwise you'll plot kernel density estimates (smoothed continuous "estimates")  over rescaled histograms bin values and they won't be normalized by the same process so would not be comparable!

Useful threads:  

https://stackoverflow.com/questions/46734555/ggplot2-histogram-why-do-y-density-and-stat-density-differ

https://stackoverflow.com/questions/65631194/scale-density-curve-made-with-geom-density-to-similar-height-of-geom-histogram

##### Single layer

```{r}
sim_optim_best %>%
  select(true_rt, true_yesChosen, reactionTime, choice, type, day) %>%
  mutate(optim_rt = ifelse(choice == "no", (-1)*reactionTime, reactionTime),
         true_rt = ifelse(true_yesChosen == 0, (-1)*true_rt, true_rt)) %>%
  select(-true_yesChosen, -reactionTime, -choice) %>%
  gather(key, value, -type, -day) %>%
  rbind(sim_grid_best %>%
  select(reactionTime, choice, type, day) %>%
  mutate(grid_rt = ifelse(choice == "no", (-1)*reactionTime, reactionTime)) %>%
  select(-reactionTime, -choice) %>%
  gather(key, value, -type, -day)) %>%
  mutate(day = paste0("Day ", day),
         day = factor(day, levels = paste0("Day ", seq(1,11, 1))),
         type = ifelse(type == 1, "HT", "RE"),
         key = factor(key, levels = c("true_rt", "grid_rt", "optim_rt"))) %>%
  ggplot(aes(value, color = key, fill = key))+
  geom_histogram(aes(y=..density..), bins = 30, alpha = .5, position = "identity")+
  facet_grid(type ~ day)+
  scale_color_manual(values = c("gray", "#7570b3", "#e7298a"))+
  scale_fill_manual(values = c("gray", NA, NA))+
    theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = "bottom")+
  labs(y = "", x = "", fill = "", color = "", title = "Single layers, same geom, stat=density")+
  ylim(0, 1.5)


```

```{r}
sim_optim_best %>%
  select(true_rt, true_yesChosen, reactionTime, choice, type, day) %>%
  mutate(optim_rt = ifelse(choice == "no", (-1)*reactionTime, reactionTime),
         true_rt = ifelse(true_yesChosen == 0, (-1)*true_rt, true_rt)) %>%
  select(-true_yesChosen, -reactionTime, -choice) %>%
  gather(key, value, -type, -day) %>%
  rbind(sim_grid_best %>%
  select(reactionTime, choice, type, day) %>%
  mutate(grid_rt = ifelse(choice == "no", (-1)*reactionTime, reactionTime)) %>%
  select(-reactionTime, -choice) %>%
  gather(key, value, -type, -day)) %>%
  mutate(day = paste0("Day ", day),
         day = factor(day, levels = paste0("Day ", seq(1,11, 1))),
         type = ifelse(type == 1, "HT", "RE"),
         key = factor(key, levels = c("true_rt", "grid_rt", "optim_rt"))) %>%
  ggplot(aes(value, color = key, fill = key))+
  # geom_step(aes(y=..density..), stat="bin", binwidth=0.2)+
    geom_step(aes(y=..density..), stat="bin", bins=30)+
  facet_grid(type ~ day)+
  scale_color_manual(values = c("gray", "#7570b3", "#e7298a"))+
  scale_fill_manual(values = c("gray", NA, NA))+
    theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = "bottom")+
  labs(y = "", x = "", fill = "", color = "", title = "Single layers, same geom, stat=density")+
  ylim(0, 1.5)

```

##### KL divergence

To compute e.g. KL divergence between true and simulated RT distributions you could do something like getting the number of values in each RT bin in the above plot using `ggplot_build(p)`

```{r}
pg = ggplot_build(p)

# Density layer data (simulated)
pg$data[[1]]
```

```{r}
# Histogram layer data (true)
hist_data = pg$data[[2]]

# ncount and ndensity are the same
sum(round(hist_data$ncount, 5) == round(hist_data$ndensity, 5)) == nrow(hist_data)

# y, density and ymax are the same
sum(round(hist_data$y, 5) == round(hist_data$density, 5)) == nrow(hist_data)
sum(round(hist_data$y, 5) == round(hist_data$ymax, 5)) == nrow(hist_data)

# is ndensity normalized density?
sort(unique( round(hist_data$density/ hist_data$ndensity, 10)))

length(unique( round(hist_data$density/ hist_data$ndensity, 10)))

hist_data %>%
  group_by(PANEL) %>%
  summarise(sum_ndens = sum(ndensity))

unique(hist_data$PANEL)

unique(hist_data$group)
```

# Other checks

Is the smaller negative log likelihoods for RE compared HT a thing for all other subjects too? Yes!

But wait. This isn't necessarily better fit. This isn't normalized. There's fewer RE trials so the sum of likelihood for each trial is smaller.

```{r}
all_optim_filtered %>%
  group_by(subnum, day, type) %>%
  filter(nll == min(nll)) %>%
  ggplot(aes(nll, fill = type))+
  geom_histogram(alpha = .5, bins = 30, position = "identity")+
  theme(legend.position = "bottom")+
  scale_fill_brewer(palette = "Dark2")+
  labs(fill = "")+
  facet_wrap(~subnum)
```

Where did the best optim estimates start from?

```{r}
all_optim_filtered %>%
  group_by(subnum, day, type) %>%
  filter(nll == min(nll))
```
